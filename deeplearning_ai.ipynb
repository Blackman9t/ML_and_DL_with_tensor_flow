{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning.ai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNst7u1Te8/Fb/+fF5NkB3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blackman9t/ML_and_DL_with_tensor_flow/blob/master/deeplearning_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUo4bW0RC8WT",
        "colab_type": "text"
      },
      "source": [
        "## Notations:\n",
        "\n",
        "1. A single training example is represented by a pair <b>$(x,y)$</b>, where $x$ is an $nx$ dimensional feature vector and $y$ the label is either 0 or 1.:- <h2>${x\\in\\ R}^{n_x}$ and $y\\in\\{0,1\\}$</h2>\n",
        "\n",
        "2. Our training set would comprise <b>$m$</b> training examples like illustrated below:<h2>$\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), (x^{(3)},y^{(3)})... (x^{(m)},y^{(m)})\\}$</h2>\n",
        "lower case $m$ will denote the number of training examples. Thus $m_{train}$ and $m_{test}$ may refer to the training and testing examples.\n",
        "\n",
        "3. Finally Matrix $X$ will be defined by taking the training examples $x^{(1)}$, $x^{(2)}$ and so on and stacking them in columns.<br>This means $x^{(1)}$ becomes the first column in Matrix $X$ and $x^{(2)}$ is 2nd column and so on, down to $x^{(m)}$.<br>Therefore this Matrix $X$, will have $m$ columns, where $m$ is the number of training examples, and the number of rows or the height of this Matrix is $nx$. Therefore:- <h2>$X\\in\\ R^{nx*m}$ Matrix</h2>\n",
        "\n",
        "4. To make our implementation of a neural network easier, after stacking the $X$ feature Matrix by columns, we need to stack the $Y$ label Matrix by columns too:- <h2>$Y = [y^{(1)}, y^{(2)}, y^{(3)} ... y^{(m)}]$</h2> like so... Therefore: <h2>$Y\\in\\ R^{1*m}$ Matrix</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flSLaae9dkDv",
        "colab_type": "text"
      },
      "source": [
        "**RELU:**\n",
        "\n",
        "RELU function stands for Rectified Linear Unit. Rectified just means taking a max of 0, that is why we get the RELU function shape.\n",
        "<br>Neural Networks(NN) are densely connectd because each input feature is connected to every hidden layer. Thus each hidden layer has inputs from all the input features.\n",
        "<br>Given input features $X$ and a target label $y$, Neural networks are remarkably good at predicting $y$ even for unseen data. NNs are certainly a lot more powerful in supervised learning settings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQTgKcLmjFI4",
        "colab_type": "text"
      },
      "source": [
        "### Basics of Neural Network Programming:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMd0RVY7jEgp",
        "colab_type": "text"
      },
      "source": [
        "When we program neural networks, we usually keep the parameters $W$ and $b$ separate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XK4csRdicAi",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression as a Neural Network:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXvVWD8giB6x",
        "colab_type": "text"
      },
      "source": [
        "The parameters of a logistic regression are $W$ and $b$.<br>$W$ is an $n_x$ dimensional vector, and $b$, is a real number. $W$ are the weights of the logistic regression and $b$ is the interceptor.\n",
        "\n",
        "Logistic Regression is a variation of Linear Regression, useful when the observed dependent variable, <b>y</b>, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n",
        "\n",
        "Logistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate into a probability with the following function, which is called sigmoid function ùúé:\n",
        "\n",
        "$$\n",
        "‚Ñé_\\theta(ùë•) = \\sigma({\\theta^TX}) =  \\frac {e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +...)}}{1 + e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +\\cdots)}}\n",
        "$$\n",
        "Or:\n",
        "$$\n",
        "ProbabilityOfaClass_1 =  P(Y=1|X) = \\sigma({\\theta^TX}) = \\frac{e^{\\theta^TX}}{1+e^{\\theta^TX}} \n",
        "$$\n",
        "\n",
        "In this equation, ${\\theta^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\\sigma(\\theta^TX)$ is the sigmoid or [logistic function](http://en.wikipedia.org/wiki/Logistic_function), also called logistic curve. It is a common \"S\" shape (sigmoid curve).\n",
        "\n",
        "So, briefly, Logistic Regression passes the input through the logistic/sigmoid but then treats the result as a probability:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBX1oM2bk445",
        "colab_type": "text"
      },
      "source": [
        "Therefore assume that $\\theta^TX + b$ = $z$,\n",
        "<br>Then the sigmoid function applied to $z$ will be<h2>$1\\over1+e^{-z}$</h2> where $e$ is the euler's number of value 2.7182818"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_zyaz_xs4Cn",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression Cost Function:**\n",
        "\n",
        "_Loss Function_: Note that the Loss function is defined with respect to a single training example. It measures how well we're doing on a single training example.\n",
        "<br>_Cost Function_: While the Cost function measures how well we're doing on the entire training set. This is the cost of all our parameters. So in training our logistic regression model, we're going to try to find parameters $W$ and $b$ that minimise the Cost function.Eventually we can see logistic regression as a very small neural network.\n",
        "\n",
        "See the notation below for the Loss and Cost functions of a logistic regression:-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4oHA4wEiWtk",
        "colab_type": "text"
      },
      "source": [
        "**Loss Function:** <h2>$LF = -(y^{(i)}*logyhat^{(i)} + (1 - y^{(i)}) * log(1 - yhat^{(i)}))$</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tTCWCe4jx57",
        "colab_type": "text"
      },
      "source": [
        "**Cost Function:** <h2>$J(W,b) = -{1\\over m} \\sum^m_{i=1} (LF)$</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chpMEg3nlWqf",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XjxCqG2MCCT",
        "colab_type": "text"
      },
      "source": [
        "While applying the Gradient Descent algorithm, the cost function is updated to be...<br>\n",
        "$J(W,b) :-$ <h2>$W:= W - \\alpha ({\\partial J(W,b) \\over \\partial W})$</h2>\n",
        "<h2>$b:= b - \\alpha ({\\partial J(W,b) \\over \\partial b})$</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A464rhCVHxcK",
        "colab_type": "text"
      },
      "source": [
        "The definition of derivative is the slope of a function at any given point. The slope is basically the the rise divided by the run, or the height divided by the width.\n",
        "<br>The optimisation function of Gradient Descent has a convex shape like below:-\n",
        "\n",
        "<img src= 'https://github.com/Blackman9t/Machine_Learning/blob/master/gradient_descent%20(1).jpg?raw=true' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqfjnxsrO221",
        "colab_type": "text"
      },
      "source": [
        "For example if the cost function is randomly initialised beween points A and B in the chart above. The GD will calculate the derivative or the slope of the point where it is initialised and this derivative will be a negative number. Then it plugs it to the formulas above for $W$ and $b$ and updates each accordingly. note that in this case by multiplying $-\\alpha$ which is the learning rate to a negative derivative, we get a positive number and its added to the cost function which now moves a step positively closer to the global minimum point B in the chart.\n",
        "<br> The opposite is the case if the cost function is randomly initialised between point B and C in the chart. In that case the derivative of the point will be a positive number. Then multiplying $-\\alpha$ to a positive number gives a negative number and this now becomes the new step as the cost function takes a negative step moving closer from region C to B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU172nUeSLyo",
        "colab_type": "text"
      },
      "source": [
        "## Vectorisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M221xBPSQ_Q",
        "colab_type": "text"
      },
      "source": [
        "**Vectorisation Demo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5-A33BggLzR",
        "colab_type": "code",
        "outputId": "48ead28d-a720-40fb-efb6-567bd5e2fbce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "a = np.random.rand(1000000)\n",
        "b = np.random.rand(1000000)\n",
        "\n",
        "tic = time.time()\n",
        "c = np.dot(a,b)\n",
        "toc = time.time()\n",
        "\n",
        "print(c)\n",
        "print('Vectorised version: ' + str(1000 * (toc - tic)), 'ms.')\n",
        "\n",
        "c = 0\n",
        "tic = time.time()\n",
        "for i in range(1000000):\n",
        "    c += a[i] * b[i]\n",
        "toc = time.time()\n",
        "\n",
        "print(c)\n",
        "print('For-Loop version: ' + str(1000 * (toc - tic)), 'ms.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250029.76759978366\n",
            "Vectorised version: 1.055002212524414 ms.\n",
            "250029.76759978523\n",
            "For-Loop version: 692.579984664917 ms.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_AcMWgCpIX6",
        "colab_type": "text"
      },
      "source": [
        "## Broadcasting Example:\n",
        "\n",
        "Let's get the sum of each column of an array and then compute the percentage of each element in each column to the sum of each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka4X2nbZppAp",
        "colab_type": "code",
        "outputId": "e82b48f2-7902-4db3-be75-700985a6d62f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "A = np.array([[56.0, 0.0, 4.4, 68.0],\n",
        "             [1.2, 104.0, 52.0, 8.0],\n",
        "             [1.8, 135.0, 99.0, 0.9]])\n",
        "A"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 56. ,   0. ,   4.4,  68. ],\n",
              "       [  1.2, 104. ,  52. ,   8. ],\n",
              "       [  1.8, 135. ,  99. ,   0.9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxho69YyttGV",
        "colab_type": "code",
        "outputId": "27d39e90-2c36-435b-b48d-6eeb1be570ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('A shape is: ',A.shape)\n",
        "print(\"A dimension is: \",A.ndim)\n",
        "type(A)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A shape is:  (3, 4)\n",
            "A dimension is:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCXqQjlzqfaU",
        "colab_type": "code",
        "outputId": "6cdffddd-9f12-498e-b49a-364a0e14befb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# let's calculate the sum of each column of A in cal\n",
        "cal = A.sum(axis=0)  #np.sum(A, axis=0)\n",
        "cal"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 59. , 239. , 155.4,  76.9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Pu_UAmsEGh",
        "colab_type": "code",
        "outputId": "8497d571-2fd8-44b6-e0dc-811a4011fd53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('Cal shape is: ',cal.shape)\n",
        "print(\"Cal dimension is: \",cal.ndim)\n",
        "type(cal)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cal shape is:  (4,)\n",
            "Cal dimension is:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHq9BSMdruUY",
        "colab_type": "code",
        "outputId": "2d28e448-95e2-45e5-8a50-5870d7514bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Let's calculate the percentage of each element to each column sum.\n",
        "percentage = 100 * (A / cal)\n",
        "percentage"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[94.91525424,  0.        ,  2.83140283, 88.42652796],\n",
              "       [ 2.03389831, 43.51464435, 33.46203346, 10.40312094],\n",
              "       [ 3.05084746, 56.48535565, 63.70656371,  1.17035111]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh268KpQrxiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_2uikoS01bR",
        "colab_type": "text"
      },
      "source": [
        "**Avoid Rank 1 Arrays:**\n",
        "\n",
        "Rank 1 arrays are arrays whose shape display something like this (5,) or (,5).\n",
        "<br>See example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NptalGTU1Ghn",
        "colab_type": "code",
        "outputId": "e4aa09a9-1fb1-4672-cfcc-7b25b2e3b095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "a = np.random.randn(5)\n",
        "print(a.shape)\n",
        "print(a)\n",
        "print(type(a))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5,)\n",
            "[-1.06084302  0.10358885 -0.14834132  0.86559636 -0.61669658]\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrFTfrwY1TZ4",
        "colab_type": "code",
        "outputId": "cf39026e-ee94-4900-bda2-c45671599975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Now let's try to transpose a and save in b.\n",
        "b = a.T\n",
        "\n",
        "print(b.shape)\n",
        "print(b)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5,)\n",
            "[-1.06084302  0.10358885 -0.14834132  0.86559636 -0.61669658]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0mSLsWZ2fek",
        "colab_type": "code",
        "outputId": "7d67b94e-770c-4369-cc5a-0b3e897b8ea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Let's multiply a by b and see what happens\n",
        "a * b"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.12538791, 0.01073065, 0.02200515, 0.74925706, 0.38031467])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2D7k4dS1h0H",
        "colab_type": "text"
      },
      "source": [
        "We see that the transpose of a, which is b is just a and it did not change. this would cause issues in programming neural networks.<br>A better option is to explicitly specify the full shape of a at the point of creation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eSZHRD5128G",
        "colab_type": "code",
        "outputId": "503d6acd-9253-42fb-9a41-a637bb3c01c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "a = np.random.randn(5,1)\n",
        "print(a.shape)\n",
        "print(a)\n",
        "print(type(a))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 1)\n",
            "[[-0.38656812]\n",
            " [-0.91290287]\n",
            " [-0.19181844]\n",
            " [-1.32287266]\n",
            " [ 1.19450848]]\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kusa-IIk2N2W",
        "colab_type": "text"
      },
      "source": [
        "Now a above is a full column vector. let's transpose a in b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvLv38Sv2S1L",
        "colab_type": "code",
        "outputId": "08652d01-997f-49a5-9097-b825791cd5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "b = a.T\n",
        "print(b.shape)\n",
        "print(b)\n",
        "print(type(b))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 5)\n",
            "[[-0.38656812 -0.91290287 -0.19181844 -1.32287266  1.19450848]]\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CeerINc21kq",
        "colab_type": "code",
        "outputId": "756c9472-9a95-42a0-c67e-b0a705672a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Let's multiply a by b and see what happens\n",
        "a * b"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.14943491,  0.35289914,  0.07415089,  0.51138039, -0.46175889],\n",
              "       [ 0.35289914,  0.83339165,  0.1751116 ,  1.20765424, -1.09047022],\n",
              "       [ 0.07415089,  0.1751116 ,  0.03679431,  0.25375137, -0.22912875],\n",
              "       [ 0.51138039,  1.20765424,  0.25375137,  1.74999206, -1.5801826 ],\n",
              "       [-0.46175889, -1.09047022, -0.22912875, -1.5801826 ,  1.4268505 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIegyvLo28wz",
        "colab_type": "text"
      },
      "source": [
        "Feel free to use assert statements to confirm shape of your arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbEMoAxJ3CMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert(a.shape == (5,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lVr95EJMZXc",
        "colab_type": "text"
      },
      "source": [
        "## Pros and Cons of Activation Functions\n",
        "\n",
        "<img src = 'https://github.com/Blackman9t/ML_and_DL_with_tensor_flow/blob/master/activation_funcs.jpg?raw=true' height=500 />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNN8cNEYMxf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}